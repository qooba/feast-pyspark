{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e49b0f3-4820-4345-b583-bd155cb3ebc0",
   "metadata": {},
   "source": [
    "# Generate Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c340d0-72ef-47e0-b646-5a219ee2f4fa",
   "metadata": {},
   "source": [
    "## Pandas dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa38a14e-31d1-4fdf-9b3c-100c9e554ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def generate_entities(size):\n",
    "    return np.random.choice(size, size=size, replace=False)\n",
    "\n",
    "def generate_data(entities, year=2021, month=10, day=1) -> pd.DataFrame:\n",
    "    n_samples=len(entities)\n",
    "    X, y = make_hastie_10_2(n_samples=n_samples, random_state=0)\n",
    "    df = pd.DataFrame(X, columns=[\"f0\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"f6\", \"f7\", \"f8\", \"f9\"])\n",
    "    df[\"y\"]=y\n",
    "    df['entity_id'] = entities\n",
    "    df['datetime'] = pd.to_datetime(\n",
    "            np.random.randint(\n",
    "                datetime(year, month, day, 0,tzinfo=timezone.utc).timestamp(),\n",
    "                datetime(year, month, day, 22,tzinfo=timezone.utc).timestamp(),\n",
    "                size=n_samples),\n",
    "        unit=\"s\", #utc=True\n",
    "    )\n",
    "    df['created'] = pd.to_datetime(\n",
    "            datetime.now(), #utc=True\n",
    "            )\n",
    "    df['month_year'] = pd.to_datetime(datetime(year, month, day, 0, tzinfo=timezone.utc), utc=True)\n",
    "    return df\n",
    "\n",
    "entities=generate_entities(1000000)\n",
    "\n",
    "entity_df = pd.DataFrame(data=entities, columns=['entity_id'])\n",
    "entity_df[\"event_timestamp\"]=datetime(2021, 1, 14, 23, 59, 42, tzinfo=timezone.utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52322d0-a9c2-466f-9bc3-941ef449af7d",
   "metadata": {},
   "source": [
    "## Create Delta Lake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d236327-d4cb-4312-b653-f82c5b5c64e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAY 1\n",
      "## GENERATED - 1.9863653182983398 s\n",
      "## DELTA CREATED - 118.46784734725952 s\n",
      "DAY 2\n",
      "## GENERATED - 2.2533488273620605 s\n",
      "## DELTA CREATED - 113.56314516067505 s\n",
      "DAY 3\n",
      "## GENERATED - 2.090444326400757 s\n",
      "## DELTA CREATED - 117.54949474334717 s\n",
      "DAY 4\n",
      "## GENERATED - 2.137775421142578 s\n",
      "## DELTA CREATED - 113.69700503349304 s\n",
      "DAY 5\n",
      "## GENERATED - 2.0107674598693848 s\n",
      "## DELTA CREATED - 112.49230170249939 s\n",
      "DAY 6\n",
      "## GENERATED - 2.04490327835083 s\n",
      "## DELTA CREATED - 116.83132553100586 s\n",
      "DAY 7\n",
      "## GENERATED - 2.12314772605896 s\n",
      "## DELTA CREATED - 114.3579614162445 s\n",
      "DAY 8\n",
      "## GENERATED - 2.1742141246795654 s\n",
      "## DELTA CREATED - 115.68657755851746 s\n",
      "DAY 9\n",
      "## GENERATED - 2.001004695892334 s\n",
      "## DELTA CREATED - 112.91505312919617 s\n",
      "DAY 10\n",
      "## GENERATED - 2.1537675857543945 s\n",
      "## DELTA CREATED - 113.79394125938416 s\n",
      "DAY 11\n",
      "## GENERATED - 2.077458620071411 s\n",
      "## DELTA CREATED - 116.54374861717224 s\n",
      "DAY 12\n",
      "## GENERATED - 2.2862818241119385 s\n",
      "## DELTA CREATED - 119.25584959983826 s\n",
      "DAY 13\n",
      "## GENERATED - 2.121596336364746 s\n",
      "## DELTA CREATED - 116.48291659355164 s\n",
      "DAY 14\n",
      "## GENERATED - 2.0689780712127686 s\n",
      "## DELTA CREATED - 114.06461930274963 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for d in range(1,15):\n",
    "    break # TMP :)\n",
    "    print(f\"DAY {d}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    data=generate_data(entities,month=1, day=d)\n",
    "    print(f\"## GENERATED - {time.time() - start_time} s\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    spark.createDataFrame(data).write.format(\"delta\").mode(\"append\").partitionBy('month_year').save(\"./dataset/all\")\n",
    "    print(f\"## DELTA CREATED - {time.time() - start_time} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd494f-7283-471a-b497-2db0293bc249",
   "metadata": {},
   "source": [
    "## Delta Lake history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea1faaf-9d0d-4e05-ac42-240da96793c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|     13|2022-02-11 01:08:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|         12|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|     12|2022-02-11 01:06:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|         11|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|     11|2022-02-11 01:04:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|         10|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|     10|2022-02-11 01:02:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          9|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      9|2022-02-11 01:00:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          8|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      8|2022-02-11 00:58:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          7|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      7|2022-02-11 00:56:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          6|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      6|2022-02-11 00:54:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          5|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      5|2022-02-11 00:52:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          4|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      4|2022-02-11 00:50:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          3|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      3|2022-02-11 00:48:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          2|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      2|2022-02-11 00:46:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          1|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      1|2022-02-11 00:44:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "|      0|2022-02-11 00:42:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|       null|  Serializable|         true|{numFiles -> 12, ...|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"./dataset/all\")\n",
    "\n",
    "fullHistoryDF = deltaTable.history()\n",
    "fullHistoryDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a4eb1-7815-4534-a47d-0ba4209d3958",
   "metadata": {},
   "source": [
    "# Feast Apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a8aff6-d355-4bb7-8480-2b8aa354b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created entity \u001b[1m\u001b[32mentity_id\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mmy_statistics\u001b[0m\n",
      "\n",
      "Created sqlite table \u001b[1m\u001b[32mrepo_my_statistics\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/feast/feature_view.py:100: DeprecationWarning: The argument 'input' is being deprecated. Please use 'batch_source' instead. Feast 0.13 and onwards will not support the argument 'input'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!rm -r .ipynb_checkpoints\n",
    "from feast.repo_operations import apply_total\n",
    "from feast.repo_config import load_repo_config\n",
    "from pathlib import Path\n",
    "\n",
    "repo = Path('/home/jovyan/feast-pyspark/feature_repo/')\n",
    "\n",
    "repo_config = load_repo_config(repo)\n",
    "apply_total(repo_config, repo, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41562e0-85f5-4e3c-8d63-a8abc4498857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5173</th>\n",
       "      <td>386</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5598</th>\n",
       "      <td>388</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6891</th>\n",
       "      <td>25</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>196</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14082</th>\n",
       "      <td>97</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993487</th>\n",
       "      <td>256</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993530</th>\n",
       "      <td>462</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996428</th>\n",
       "      <td>391</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996937</th>\n",
       "      <td>200</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999071</th>\n",
       "      <td>295</td>\n",
       "      <td>2021-01-14 23:59:42+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity_id           event_timestamp\n",
       "5173          386 2021-01-14 23:59:42+00:00\n",
       "5598          388 2021-01-14 23:59:42+00:00\n",
       "6891           25 2021-01-14 23:59:42+00:00\n",
       "7249          196 2021-01-14 23:59:42+00:00\n",
       "14082          97 2021-01-14 23:59:42+00:00\n",
       "...           ...                       ...\n",
       "993487        256 2021-01-14 23:59:42+00:00\n",
       "993530        462 2021-01-14 23:59:42+00:00\n",
       "996428        391 2021-01-14 23:59:42+00:00\n",
       "996937        200 2021-01-14 23:59:42+00:00\n",
       "999071        295 2021-01-14 23:59:42+00:00\n",
       "\n",
       "[501 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edf = entity_df[entity_df.entity_id<=500]\n",
    "edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96056fe0-c352-4cd6-9133-2c4fb2abf4bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "`/home/jovyan/feast-spark/feature_repo/dataset/all` is not a Delta table.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m store \u001b[38;5;241m=\u001b[39m FeatureStore(repo_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m training_df \u001b[38;5;241m=\u001b[39m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_historical_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f7\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:f9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_statistics:y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m seconds ---\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time))\n\u001b[1;32m     31\u001b[0m training_df\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/infra/offline_stores/offline_store.py:77\u001b[0m, in \u001b[0;36mRetrievalJob.to_df\u001b[0;34m(self, validation_reference)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_df\u001b[39m(\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m, validation_reference: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidationReference\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Return dataset as Pandas DataFrame synchronously including on demand transforms\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m        validation_reference: If provided resulting dataset will be validated against this reference profile.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     features_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_df_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_demand_feature_views:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# TODO(adchia): Fix requirement to specify dependent feature views in feature_refs\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m odfv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_demand_feature_views:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/usage.py:280\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mtraceback \u001b[38;5;241m=\u001b[39m _trace_to_log(traceback)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m traceback:\n\u001b[0;32m--> 280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/feast/usage.py:269\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m ctx\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mupdate(attrs)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mexception:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;66;03m# exception was already recorded\u001b[39;00m\n",
      "File \u001b[0;32m~/feast-pyspark/feast_pyspark/spark.py:76\u001b[0m, in \u001b[0;36mSparkRetrievalJob._to_df_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;129m@log_exceptions_and_usage\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_df_internal\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Only execute the evaluation function to build the final historical retrieval dataframe at the last moment.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/feast-pyspark/feast_pyspark/spark.py:173\u001b[0m, in \u001b[0;36mSparkOfflineStore.get_historical_features.<locals>.evaluate_historical_retrieval\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m right_entity_key_columns \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m right_entity_key_columns \u001b[38;5;28;01mif\u001b[39;00m c]\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# feature_view.batch_source.s3_endpoint_override\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m df_to_join \u001b[38;5;241m=\u001b[39m \u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSparkOfflineStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoDF()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Build a list of all the features we should select from this source\u001b[39;00m\n\u001b[1;32m    178\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/tmp/spark-66968efb-253e-4fb3-b49e-2545f38f98fe/userFiles-e377ce86-0002-4891-8278-b9158e6abd95/io.delta_delta-core_2.12-1.1.0.jar/delta/tables.py:350\u001b[0m, in \u001b[0;36mDeltaTable.forPath\u001b[0;34m(cls, sparkSession, path)\u001b[0m\n\u001b[1;32m    347\u001b[0m jvm: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJVMView\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    348\u001b[0m jsparkSession: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m sparkSession\u001b[38;5;241m.\u001b[39m_jsparkSession  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m jdt \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtables\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeltaTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DeltaTable(sparkSession, jdt)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: `/home/jovyan/feast-spark/feature_repo/dataset/all` is not a Delta table."
     ]
    }
   ],
   "source": [
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from feast_pyspark import SparkOfflineStore\n",
    "\n",
    "SparkOfflineStore.spark = spark\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "start_time = time.time()\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=edf, \n",
    "    features = [\n",
    "        'my_statistics:f0',\n",
    "        'my_statistics:f1',\n",
    "        'my_statistics:f2',\n",
    "        'my_statistics:f3',\n",
    "        'my_statistics:f4',\n",
    "        'my_statistics:f5',\n",
    "        'my_statistics:f6',\n",
    "        'my_statistics:f7',\n",
    "        'my_statistics:f8',\n",
    "        'my_statistics:f9',\n",
    "        'my_statistics:y',\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "training_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
